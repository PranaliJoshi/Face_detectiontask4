{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Collection Completed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "face_classifier = cv2.CascadeClassifier('C:/Users/Lenovo/Desktop/mlops-ws/haarcascade_frontalface_default.xml')\n",
    "def face_extractor(img):\n",
    "    faces = face_classifier.detectMultiScale(img,1.3,5)\n",
    "    if faces is ():\n",
    "        return None\n",
    "    for(x,y,w,h) in faces:\n",
    "          cropped_face = img[y:y+h , x:x+w]\n",
    "    return  cropped_face\n",
    "cap = cv2.VideoCapture(0)\n",
    "count = 0\n",
    "while True:\n",
    "        ret,frame = cap.read()\n",
    "        if face_extractor(frame) is not None:\n",
    "            count +=1\n",
    "            face = cv2.resize(face_extractor(frame), (224,224))\n",
    "            file_name_path = 'C://Users//Lenovo//Desktop//imagedata//train//n0//'  + str(count) + '.jpg'\n",
    "            cv2.imwrite(file_name_path , face)\n",
    "            cv2.putText(face, str(count) , (100,100) , cv2.FONT_HERSHEY_COMPLEX , 1, (0,255,0) , 2)\n",
    "            cv2.imshow('face cropper',face)\n",
    "        else:\n",
    "            print(\"Face not found\")\n",
    "            pass\n",
    "        if cv2.waitKey(1) == 13 or count == 200:\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Sample Collection Completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 InputLayer False\n",
      "1 Conv2D True\n",
      "2 Conv2D True\n",
      "3 MaxPooling2D True\n",
      "4 Conv2D True\n",
      "5 Conv2D True\n",
      "6 MaxPooling2D True\n",
      "7 Conv2D True\n",
      "8 Conv2D True\n",
      "9 Conv2D True\n",
      "10 MaxPooling2D True\n",
      "11 Conv2D True\n",
      "12 Conv2D True\n",
      "13 Conv2D True\n",
      "14 MaxPooling2D True\n",
      "15 Conv2D True\n",
      "16 Conv2D True\n",
      "17 Conv2D True\n",
      "18 MaxPooling2D True\n"
     ]
    }
   ],
   "source": [
    "for (i,layer) in enumerate(model.layers):\n",
    "    print(str(i) + \" \" + layer.__class__.__name__, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addTopModel(bottom_model, num_classes , D = 256):\n",
    "    top_model = bottom_model.output\n",
    "    top_model = Flatten(name = \"flatten\")(top_model)\n",
    "    top_model = Dense(D , activation = 'relu')(top_model)\n",
    "    top_model = Dropout(0.3)(top_model)\n",
    "    top_model = Dense(num_classes, activation =\"softmax\")(top_model)\n",
    "    return top_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_2:0' shape=(?, 224, 224, 3) dtype=float32>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x25a89f7ef88>,\n",
       " <keras.layers.convolutional.Conv2D at 0x25a89f7e948>,\n",
       " <keras.layers.convolutional.Conv2D at 0x25a89f80588>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x25a940e34c8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x25a8d445348>,\n",
       " <keras.layers.convolutional.Conv2D at 0x25a936219c8>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x25aa65cadc8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x25aa65bc188>,\n",
       " <keras.layers.convolutional.Conv2D at 0x25aa65c9988>,\n",
       " <keras.layers.convolutional.Conv2D at 0x25aa8421bc8>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x25aa845ef88>,\n",
       " <keras.layers.convolutional.Conv2D at 0x25aa84529c8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x25aa8daa9c8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x25aa8dffa48>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x25aa8e2d608>,\n",
       " <keras.layers.convolutional.Conv2D at 0x25aa8deab88>,\n",
       " <keras.layers.convolutional.Conv2D at 0x25aa8e45848>,\n",
       " <keras.layers.convolutional.Conv2D at 0x25aae78ad48>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x25abcad9348>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               6422784   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 21,137,986\n",
      "Trainable params: 21,137,986\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers import ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "FC_Head = addTopModel(model, num_classes)\n",
    "\n",
    "modelnew = Model(inputs = model.input, outputs = FC_Head)\n",
    "\n",
    "print(modelnew.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = 'C:/Users/Lenovo/Desktop/imagedata/train/'\n",
    "validation_data_dir = 'C:/Users/Lenovo/Desktop/imagedata/test/'\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale = 1./255,\n",
    "        rotation_range = 20,\n",
    "        width_shift_range = 0.2,\n",
    "        height_shift_range = 0.2,\n",
    "        horizontal_flip = True,\n",
    "        fill_mode = 'nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_datagen = ImageDataGenerator(rescale = 1./225)\n",
    "train_batchsize = 16\n",
    "val_batchsize = 10\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "               train_data_dir,\n",
    "               target_size = (img_rows,img_cols),\n",
    "               batch_size = train_batchsize,\n",
    "               class_mode = 'categorical')\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "              validation_data_dir,\n",
    "              target_size = (img_rows, img_cols),\n",
    "              batch_size = val_batchsize,\n",
    "              class_mode = 'categorical',\n",
    "              shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "74/74 [==============================] - 1073s 14s/step - loss: 3246.3448 - accuracy: 0.4688 - val_loss: 0.6375 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63747, saving model to face_recognition.h5\n",
      "Epoch 2/3\n",
      "74/74 [==============================] - 1072s 14s/step - loss: 0.6938 - accuracy: 0.4797 - val_loss: 0.6957 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.63747\n",
      "Epoch 3/3\n",
      "74/74 [==============================] - 1115s 15s/step - loss: 0.6934 - accuracy: 0.4772 - val_loss: 0.6900 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.63747\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "                   \n",
    "checkpoint = ModelCheckpoint(\"face_recognition.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 3,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callbacks = [earlystop, checkpoint]\n",
    "\n",
    "# Note we use a very small learning rate \n",
    "modelnew.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = RMSprop(lr = 0.001),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "nb_train_samples = 1190\n",
    "nb_validation_samples = 170\n",
    "epochs = 3\n",
    "batch_size = 16\n",
    "\n",
    "history = modelnew.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = nb_train_samples // batch_size,\n",
    "    epochs = epochs,\n",
    "    callbacks = callbacks,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_validation_samples // batch_size)\n",
    "\n",
    "modelnew.save(\"face_recognition.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "classifier = load_model('face_recognition.h5')\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "face_recognition = {\"[0]\": \"Pranali Joshi\", \n",
    "                    \"[1]\": \"Pranali's Mother\"}\n",
    "face_recognition_n = {\"n0\": \"Pranali Joshi\", \n",
    "                      \"n1\": \"Pranali's Mother\"}\n",
    "def draw_test(name, pred, im):\n",
    "    face = face_recognition[str(pred)]\n",
    "    BLACK = [0,0,0]\n",
    "    expanded_image = cv2.copyMakeBorder(im, 80, 0, 0, 100 ,cv2.BORDER_CONSTANT,value=BLACK)\n",
    "    cv2.putText(expanded_image, face, (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,255), 2)\n",
    "    cv2.imshow(name, expanded_image)\n",
    "\n",
    "def getRandomImage(path):\n",
    "    \"\"\"function loads a random images from a random folder in our test path \"\"\"\n",
    "    folders = list(filter(lambda x: os.path.isdir(os.path.join(path, x)), os.listdir(path)))\n",
    "    random_directory = np.random.randint(0,len(folders))\n",
    "    path_class = folders[random_directory]\n",
    "    print(\"Class - \" + face_recognition_n[str(path_class)])\n",
    "    file_path = path + path_class\n",
    "    file_names = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
    "    random_file_index = np.random.randint(0,len(file_names))\n",
    "    image_name = file_names[random_file_index]\n",
    "    return cv2.imread(file_path+\"/\"+image_name)    \n",
    "\n",
    "for i in range(0,10):\n",
    "    input_im = getRandomImage(\"C:/Users/Lenovo/Desktop/imagedata/train/\")\n",
    "    input_original = input_im.copy()\n",
    "    input_original = cv2.resize(input_original, None, fx=0.5, fy=0.5, interpolation = cv2.INTER_LINEAR)\n",
    "    input_im = cv2.resize(input_im, (224, 224), interpolation = cv2.INTER_LINEAR)\n",
    "    input_im = input_im / 255.\n",
    "    input_im = input_im.reshape(1,224,224,3) \n",
    "    res = np.argmax(classifier.predict(input_im, 1, verbose = 0), axis=1) \n",
    "    draw_test(\"Prediction\", res, input_original) \n",
    "    cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
